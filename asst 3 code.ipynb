{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neural Network\n",
    "### Catherine Al Aswad (305541)\n",
    "### CS 4120 Assignment 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Most Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "isY9debrWW2I"
   },
   "outputs": [],
   "source": [
    "# Reading in the necessary packages and setting a seed.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyWxGqTMYJ6J"
   },
   "outputs": [],
   "source": [
    "# activFunc takes a numeric input x so that x can be inputed into a function 0,1,2,3,4\n",
    "# option == 0 : plugs x into a sigmoid function\n",
    "# option == 1 : plugs x into a tanh function\n",
    "# option == 2 : plugs x into a ReLu function\n",
    "# option == 3 : plugs x into a leaky ReLu function\n",
    "# option == 4 : plugs x into a softmax function\n",
    "# returns 'a' for the chosen function\n",
    "def activFunc(x, option):\n",
    "    a = 0\n",
    "    if option == 0:           # sigmoid\n",
    "        a = 1/(1+np.exp(-x))\n",
    "        \n",
    "    elif option == 1:    # tanh\n",
    "        a = (np.exp(x) - np.exp(-x))/(np.exp(x) +np.exp(-x))\n",
    "   \n",
    "\n",
    "    elif option == 2:   # ReLu\n",
    "        a = np.maximum(0,x)\n",
    "            \n",
    "    elif option == 3:   # leaky ReLu\n",
    "        a = np.maximum(0.01*x,x)\n",
    "        \n",
    "    elif option == 4:    # softmax\n",
    "        a = np.exp(x)/ np.exp(x).sum(axis = 0) \n",
    "\n",
    "    return a\n",
    "\n",
    "# activFuncDeriv takes a numeric input x so that x can be inputed into a function 0,1,2,3\n",
    "# option == 0 : plugs x into the derivative of the sigmoid function\n",
    "# option == 1 : plugs x into the derivative of the tanh function\n",
    "# option == 2 : plugs x into the derivative of the ReLu function\n",
    "# option == 3 : plugs x into the derivative of the leaky ReLu function\n",
    "# returns 'da' for the chosen function\n",
    "def activFuncDeriv(x, option):\n",
    "    da = 0\n",
    "    if option == 0:           # sigmoid derivative\n",
    "        da = x - np.power(x, 2)\n",
    "        \n",
    "    elif option == 1:    # tanh derivative\n",
    "        da =  1 - np.power(x, 2)\n",
    "        \n",
    "    elif option == 2:   # ReLu derivative,  x>0 then it is 1, otherwize 0\n",
    "        da = np.greater(x, 0).astype(int)\n",
    "\n",
    "    elif option == 3:   # leaky ReLu derivative\n",
    "        da = np.where(-5 <= 0, 0.01, 1)\n",
    "        \n",
    "    return da\n",
    "\n",
    "# load_mnist_dataset: loads the mnist dataset. \n",
    "# The X features are scaled\n",
    "# The target Y is changed from one column to 10 columns, one for each digit 0 to 9, to create a multiclass label\n",
    "# The data is split into 40% test data and 60% train data \n",
    "# The method returns the feautures and labels of the train and test data\n",
    "def load_mnist_dataset():\n",
    "    np.random.seed(1)\n",
    "    mnist = fetch_openml('mnist_784', version=1)\n",
    "    # print(mnist.keys())\n",
    "    X = mnist['data'].astype(np.float32)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    Y = mnist['target'].astype(np.float32)\n",
    "    multilabel = OneHotEncoder(sparse=False)\n",
    "    Y = multilabel.fit_transform(Y.reshape(-1, 1))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=4)\n",
    "\n",
    "    \n",
    "    return X_train.T, X_test.T, y_train.T, y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "PDjwGSqfWW2N",
    "outputId": "ce691d31-7350-4794-cf2a-0733f755cae8"
   },
   "outputs": [],
   "source": [
    "# loading the mnist dataset and creating the test and train datasets\n",
    "X_train, X_test, y_train, y_test = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_z1p1s7tWW2h"
   },
   "outputs": [],
   "source": [
    "# init_params initializes the parameters of our network to small non-zero random weights\n",
    "# n_x represents the number of input features\n",
    "# n_h represents the number of hidden units (in the hidden layer)\n",
    "# n_y represents the number of output units\n",
    "# n_hl represents the number of hidden layers; there is at least 2 hidden layers\n",
    "# The function can initialize weights for more than 2 layers - for as much as inducated by n_hl\n",
    "# The function returns a dictionary params that stores the weights and b's\n",
    "def init_params(n_x, n_h, n_y, n_hl):\n",
    "    \n",
    "    params = {}\n",
    "    np.random.seed(2)\n",
    "    \n",
    "    # first hidder layer\n",
    "    W1 = np.random.randn(n_h, n_x)* 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    params[\"W1\"] = W1\n",
    "    params[\"b1\"] = b1\n",
    "\n",
    "    # The size of the weights and b matrices for the 2nd to the n_hl - 1 hidder layer are the same in this simple neural network\n",
    "    for i in range(2,n_hl):\n",
    "        W = np.random.randn(n_h, n_h)* 0.01\n",
    "        b = np.zeros((n_h, 1))\n",
    "        params[\"W\"+str(i)] = W\n",
    "        params[\"b\"+str(i)] = b\n",
    "    \n",
    "    # last hidder layer\n",
    "    Wn = np.random.randn(n_y, n_h)* 0.01\n",
    "    bn = np.zeros((n_y, 1))\n",
    "    params[\"W\"+str(n_hl)] = Wn\n",
    "    params[\"b\"+str(n_hl)] = bn\n",
    "     \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lyiw6PqdWW2n"
   },
   "outputs": [],
   "source": [
    "# This function performs forward propagation\n",
    "# it receives as parameters the matrix X containing the input features for the entire training set\n",
    "# and the paramters of the network in the dictionary params\n",
    "# actFun specifies which activation function to use and takes a value of 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "# Includes inverted dropout (for regularization) with a default keep_prob = 0.6, so 60% of the units will be kept\n",
    "# Applied to as many layers as the number of weights in the params dictionary, and can be greater than or equal to 2 layers\n",
    "# The softmax function is applied to the last layer of the neural network \n",
    "#       to calculate the final output (adapting for multiclass classification).\n",
    "# The function returns the final output A_temp, with a dictionary containig all Zi and Ai matrices\n",
    "def forward_propagation(X, params, actFun, keep_prob = 0.6):\n",
    "    \n",
    "    forwd = {}\n",
    "    \n",
    "    # first hidder layer\n",
    "    W1 = params['W1']\n",
    "    b1 = params['b1']\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = activFunc(Z1, actFun)  \n",
    "    forwd[\"Z1\"] = Z1\n",
    "    forwd[\"A1\"] = A1\n",
    "    A_temp = A1\n",
    "    \n",
    "    for i in range(2, int(len(params)/2)):      # hidder layer 2 to lastHiddenLayer-1\n",
    "        W = params['W'+str(i)]\n",
    "        b = params['b'+str(i)]\n",
    "        Z = np.dot(W, A_temp) + b\n",
    "        A = activFunc(Z, actFun)\n",
    "        drop = np.random.randn(A.shape[0], A.shape[1]) < keep_prob   # applying inverted dropout (not applied on 1st and last layer)\n",
    "        A = (np.multiply(A, drop))/keep_prob\n",
    "        \n",
    "        forwd[\"Z\"+str(i)] = Z\n",
    "        forwd[\"A\"+str(i)] = A\n",
    "        A_temp = A\n",
    "\n",
    "    # last hidder layer\n",
    "    Wn = params['W'+str(int(len(params)/2))]\n",
    "    bn = params['b'+str(int(len(params)/2))]\n",
    "    Zn = np.dot(Wn, A_temp) + bn\n",
    "    An = activFunc(Zn, 4)  # softmax layer\n",
    "    forwd[\"Z\"+str(int(len(params)/2))] = Zn\n",
    "    forwd[\"A\"+str(int(len(params)/2))] = An\n",
    "    A_temp = An\n",
    "\n",
    "    return A_temp, forwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Y_101YQWW2t"
   },
   "outputs": [],
   "source": [
    "# Here we compute the cost function over the entire training set\n",
    "# all we need is the predicted value by the network (Y_pred) \n",
    "# and the actual class of the training examples (Y)\n",
    "def compute_cost(Y_pred, Y):\n",
    "    m = Y.shape[1] # number of example\n",
    "    logprobs = np.multiply(np.log(Y_pred),Y) + np.multiply(np.log(1 - Y_pred), (1 - Y))\n",
    "    cost = - (1/m) * np.sum(logprobs) \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is a real number.\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9VtgD8hWW2y"
   },
   "outputs": [],
   "source": [
    "# This function performs backward propagation\n",
    "# it receives as parameters the matrix X containing the input features for the entire training set\n",
    "# and the paramters of the network in the dictionary params\n",
    "# actFun specifies which activation function to use and takes a value of 0 (sigmid drivative) ,1(tanh drivative),\n",
    "#        2 (relu drivative),3 (leaky relu drivative)\n",
    "# Y is the observed labels for the trainign dataset, and \n",
    "# forwd is the dictionary that contains the Z ans A matrices.\n",
    "# Applied to as many layers as the number of weights in the params dictionary, and can be greater than or equal to 2 layers\n",
    "# The function returns a dictionary grads of the gradients dWi and dbi for all the layers\n",
    "def backward_propagation(params, forwd, X, Y, actFun):\n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    \n",
    "    An = forwd['A'+str(int(len(forwd)/2))]\n",
    "    dZn = An - Y  #dz in slide 10, result of applying the chain rule (see also slide 18)\n",
    "    \n",
    "    dZ_current = dZn     \n",
    "    for i in range(int(len(params)/2),1,-1):     \n",
    "        Ak_next = forwd['A'+str(i-1)]\n",
    "        dW = 1/m*np.dot(dZ_current, Ak_next.T)                 # dW for last hidder layers to 2nd hidder layer\n",
    "        db = 1/m*np.sum(dZ_current, axis=1, keepdims=True)     # db for last hidder layers to 2nd hidder layer\n",
    "        grads[\"dW\"+str(i)] = dW\n",
    "        grads[\"db\"+str(i)] = db\n",
    "        \n",
    "        Wcurrent = params['W'+str(i)]\n",
    "        dZ_next = np.multiply(np.dot(Wcurrent.T, dZ_current), activFuncDeriv(Ak_next , actFun))   # applies the derivative of the activation function\n",
    "        dZ_current = dZ_next\n",
    "        \n",
    "    dW1 = 1/m*np.dot(dZ_current, X.T)     # first hidder layer\n",
    "    db1 = 1/m*np.sum(dZ_current, axis=1, keepdims=True)\n",
    "    grads[\"dW1\"] = dW1\n",
    "    grads[\"db1\"] = db1\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i0tteT9VWW3E"
   },
   "outputs": [],
   "source": [
    "# This function uses the gradients and the learning rate to update the parameters\n",
    "# learn_rate = 1.2 is the default\n",
    "# params is a dictionary containing the weight and b's of the layers\n",
    "# Applied to as many layers as the number of weights in the params dictionary, and can be greater than or equal to 2 layers\n",
    "# returns a dictionary of the updated parameters\n",
    "def update_params(params, grads, learn_rate = 1.2):\n",
    "    params2 = {}\n",
    "    for i in range(1,int(len(params)/2)+1):\n",
    "        W = params['W'+str(i)]\n",
    "        b = params['b'+str(i)]\n",
    "        dW = grads['dW'+str(i)]\n",
    "        db = grads['db'+str(i)]\n",
    "        W = W - learn_rate*dW\n",
    "        b = b - learn_rate*db\n",
    "        params2[\"W\"+str(i)] = W\n",
    "        params2[\"b\"+str(i)] = b\n",
    "        \n",
    "    return params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0_yKzDzWW3H"
   },
   "outputs": [],
   "source": [
    "# Here we create and train the actual Neural Network model\n",
    "# We receive the dataset features X and classes Y\n",
    "# we receive the number of hidden units as a hyperparameter (n_h)\n",
    "# and we also get as a hyperparameter how many iterations to train\n",
    "# actFun specified the activation function 0 (sigmid) ,1(tanh),2(relu),3(leaky relu), default is sigmoid function\n",
    "# n_hl represents the number of hidden layers; there is at least 2 hidden layers\n",
    "# keep_prob for tuning the degree of regularization for the inverted dropout regularization, with a 0.6 default value\n",
    "# learn_rate for tuning the learning rate of the model, set to a default rate of 3\n",
    "# retruns the learnt parameters params of the model\n",
    "def nn_model(X, Y, n_h, actFun = 0, num_iterations = 1, print_cost=False, n_hl=2, keep_prob=0.6 , learn_rate = 3):\n",
    "    np.random.seed(3)\n",
    "    n_x = X.shape[0]\n",
    "    n_y = Y.shape[0] \n",
    "\n",
    "    params = init_params(n_x, n_h, n_y, n_hl)\n",
    "\n",
    "    # This loop is to perform the forward and backward iterations\n",
    "    for i in range(0, num_iterations):\n",
    "        # Inside the loop all the computations (forward and backward computations) are vectorized\n",
    "        A2, forwd = forward_propagation(X, params, actFun, keep_prob)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        grads = backward_propagation(params, forwd, X, Y, actFun)\n",
    "        params = update_params(params, grads, learn_rate)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxRn8nNlWW3J"
   },
   "outputs": [],
   "source": [
    "# This method uses the learned parameters params and a set of input values X to perform a prediction \n",
    "# using activation function actFun \n",
    "# and default regularization hyperparameter keep_prob=0.6\n",
    "def predict(params, X, actFun, keep_prob=0.6):\n",
    "    Y_pred, forwd = forward_propagation(X, params, actFun, keep_prob)\n",
    "    \n",
    "    # turns to a hardmax layer: the digit with the highest probability for a given observation\n",
    "    # is turned to 1, while the other digit probabilities are replaces with a probability of 0 \n",
    "    predictions = (Y_pred == Y_pred.T.max(axis=1)[:,None].T).astype(int)  > 0.5\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "c4wKevCGWW3O",
    "outputId": "33131c80-bbda-4662-d00d-d55e5c972d68"
   },
   "outputs": [],
   "source": [
    "# This function returns a table with the accuracy score (in %) of predicting each digit, the overall average accuracy%, and the \n",
    "# mean squared error between the predicted and observed labels.\n",
    "# modelLabel gives the data summary a model name to be associated with once it is returned\n",
    "def getAccuracy_MSE (pred, labels, modelLabel = ''):\n",
    "    totalAccuracy = 0\n",
    "    # percentages\n",
    "    Acc_MSE = pd.DataFrame(columns  = ['0','1','2','3','4','5','6','7','8','9','TotalAcc','MSE', 'Model','dataset'])\n",
    "\n",
    "    for i in range(0,10):   # for all 10 classes  0 to 9\n",
    "        newAccuracy = float((np.dot(labels[i],pred[i].T))) + np.dot(1-labels[i],1-pred[i].T)\n",
    "        totalAccuracy = totalAccuracy + newAccuracy\n",
    "        Acc_MSE.at[0, str(i)] = round(float(newAccuracy/float(labels[i].size)*100), 1)  \n",
    "        \n",
    "    Acc_MSE.at[0, 'TotalAcc'] = round(float(totalAccuracy/float(labels.T.size)*100), 1)    # avergae accuracy\n",
    "    Acc_MSE.at[0, 'MSE'] = round(metrics.mean_squared_error(labels, pred), 4)       # MSE\n",
    "    Acc_MSE.at[0, 'Model'] = modelLabel\n",
    "    \n",
    "    return Acc_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelStructure : creates a table with summarized information for a neural network it fits with different structures\n",
    "# function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "# HU_LowerLimit : lower limit for the number of hidden units in a layer\n",
    "# HL_LowerLimit : lower limit for the number of hidden layers\n",
    "# learnRate : learning rate \n",
    "# keepProb : inverted dropout regularization parameter\n",
    "# nbIterations : number of iterations to use to train the model\n",
    "# Modelsummary : table that contains the basic information about the neural network structure, \n",
    "#                 the accuracy scores, and MSE scores for the train and test data\n",
    "\n",
    "def modelStructure(function, HU_LowerLimit, HL_LowerLimit, learnRate, keepProb, nbIterations):\n",
    "    \n",
    "    # contains the basic information about the neural network structure\n",
    "    # Model :  model name signified by a number (or model index)\n",
    "    # dataset : wether the information in the row is for the training or test data\n",
    "    # HU : number of hidden units in a layer\n",
    "    # HL : number of hidden layers\n",
    "    # LR : learning rate\n",
    "    # nbIter : number of iterations\n",
    "    # KP : keepProb : inverted dropout regularization parameter\n",
    "    # Function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "    Modelsummary = pd.DataFrame(columns  = ['Model','dataset', 'HU','HL', 'LR','nbIter','KP','Function'])\n",
    "    \n",
    "    # will contain the accuracy scores and MSE scores for the train and test data\n",
    "    AnalysisSummary = pd.DataFrame()\n",
    "\n",
    "    for k in range(0,6):    # training 6 models with randomized number of hiddenUnits and hiddenLayers\n",
    "        hiddenUnits = np.random.randint(HU_LowerLimit,HU_LowerLimit+40)\n",
    "        hiddenLayers = np.random.randint(HL_LowerLimit,HL_LowerLimit+40)\n",
    "        Modelsummary.loc[len(Modelsummary.index)] = [str(k),'train', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "        Modelsummary.loc[len(Modelsummary.index)] = [str(k),'test', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "\n",
    "        # training model\n",
    "        params = nn_model(X_train, y_train, n_h = hiddenUnits, num_iterations = nbIterations, print_cost=True, actFun = function, n_hl=hiddenLayers, keep_prob=keepProb, learn_rate = learnRate)\n",
    "\n",
    "        # Computing the total accuracy (%), accuracy for each digit, and MSE\n",
    "        tr_pred = predict(params, X_train, function, keep_prob = keepProb)    # train data\n",
    "        tr = getAccuracy_MSE(tr_pred, y_train, str(k))\n",
    "        tr.at[0, 'dataset'] = 'train'\n",
    "\n",
    "        te_pred = predict(params, X_test, function, keep_prob = keepProb)    # test data\n",
    "        te = getAccuracy_MSE(te_pred, y_test, str(k))\n",
    "        te.at[0, 'dataset'] = 'test'\n",
    "\n",
    "        AnalysisSummary = pd.concat([AnalysisSummary, (pd.concat([tr, te], axis=0))], axis=0)                        \n",
    "    \n",
    "    Modelsummary = pd.merge(Modelsummary, AnalysisSummary, on = ['Model','dataset'])\n",
    "        \n",
    "    return Modelsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Preliminary NN Structure Exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large number of hyperparamters: activation function type, number of iterations, learning rate, number of hidden layers, number of hidden units, inverted dropout regularization paramter. Due to time constraints and the use of an average computer with limited space, a grid search cannot be done and completed in time. So, an alterative method of exploring hyperparamter combinations is adopted.\n",
    "\n",
    "In an attempt to decide on the best structure for the neural network, some simple models are explored. First, 6 models are fitted with the sigmoid function, with a randomized number of hidden units (range from 4 to 44), a randomized number of hidden layers (range from 4 to 42), learnRate = 3, inverted dropdout drops 40% of the units, and 10 iterations. The information is summarized below for the train and test data. Note that the model was not fit with the test data, but the row of the test data does show the neural network structure characteristics they were used on. With a small number of iterations, we see that the models already fit the train and test data very well. The MSE for the train and test data are very small and close to eachother, which shows there may not be any overfitting or underfitting. The models have an approximate total accuracy of 82%. The digit '1' has the lowest accuracy for all the models. There is no significant difference between the prediction accuracies of the models, even though they have different numbers of hidden units and layers.\n",
    "\n",
    "Note that the cost for each of these models is high, so we will later try to increase the number of iterations to try and reduce the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250903\n",
      "Cost after iteration 0: 3.250913\n",
      "Cost after iteration 0: 3.252495\n",
      "Cost after iteration 0: 3.252330\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.251373\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>16.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>84.4</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>36</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>16.5</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>84.6</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>29</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>71.4</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>28.9</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>71.5</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train   7  32  3     10  0.6        0  90.1  11.2  90.1  89.8   \n",
       "1      0    test   7  32  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "2      1   train   8  16  3     10  0.6        0  90.1  11.2  90.1  89.8   \n",
       "3      1    test   8  16  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "4      2   train  36  27  3     10  0.6        0  90.1  16.3  90.1  89.8   \n",
       "5      2    test  36  27  3     10  0.6        0  90.1  16.5  89.9  89.8   \n",
       "6      3   train  25  20  3     10  0.6        0  90.1  11.2  90.1  89.8   \n",
       "7      3    test  25  20  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "8      4   train   4  23  3     10  0.6        0  90.1  11.2  90.1  89.8   \n",
       "9      4    test   4  23  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "10     5   train  30  35  3     10  0.6        0  90.1    29  90.1  89.8   \n",
       "11     5    test  30  35  3     10  0.6        0  90.1  28.9  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  84.4  90.3  90.1     82.3  0.1775  \n",
       "5   90.4  90.8  90.1  84.6  90.2  89.9     82.3  0.1775  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  71.4  90.3  90.1     82.2  0.1778  \n",
       "11  90.4  90.8  90.1  71.5  90.2  89.9     82.2  0.1783  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 0    # sigmoid\n",
    "hiddenUnits_LL = 4\n",
    "hiddenLayers_LL = 2\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10  \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same neural network characteristics as above, we try to use a tanh activation function instead. (The number of hidden units and layers are still randomized here). The information is summarized below for the train and test data. With a small number of iterations, we see that the models already fit the train and test data very well. The MSE for the train and test data are very small and close to eachother, which shows there may not be any overfitting or underfitting. The models have an approximate total accuracy of 82%. The digit '1' has the lowest accuracy for all the models. These observations are very similar to the observations using the sigmoid activation function.\n",
    "\n",
    "A significant observation that we can mention is about model 1 (2nd model). This model has an accuracy of about 82.7% with a lower MSE score and a slightly smaller cost than the other models. What makes this model different than the others is that it has a large number of hidden units, and a relalively small number of hidden layers. All the other models that use the sigmoid and tanh activation functions do not have this structural property. Later, when we try to improve the model, we can explore a large number of hidden units, and a relalively small number of hidden layers.\n",
    "\n",
    "Note that the cost for each of these models is high, so we will later try to increase the number of iterations to try and reduce the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250646\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>75.6</td>\n",
       "      <td>86</td>\n",
       "      <td>89.8</td>\n",
       "      <td>88.5</td>\n",
       "      <td>85.6</td>\n",
       "      <td>76.6</td>\n",
       "      <td>84.5</td>\n",
       "      <td>71.9</td>\n",
       "      <td>79.5</td>\n",
       "      <td>82.7</td>\n",
       "      <td>0.173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>75.7</td>\n",
       "      <td>85.8</td>\n",
       "      <td>89.8</td>\n",
       "      <td>88.7</td>\n",
       "      <td>85.5</td>\n",
       "      <td>76.6</td>\n",
       "      <td>85</td>\n",
       "      <td>71.8</td>\n",
       "      <td>79.3</td>\n",
       "      <td>82.7</td>\n",
       "      <td>0.1728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  29  32  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "1      0    test  29  32  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "2      1   train  42   3  3     10  0.6        1    89  75.6    86  89.8   \n",
       "3      1    test  42   3  3     10  0.6        1    89  75.7  85.8  89.8   \n",
       "4      2   train  16  26  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "5      2    test  16  26  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "6      3   train  19   5  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "7      3    test  19   5  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "8      4   train  26  30  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "9      4    test  26  30  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "10     5   train  11  31  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "11     5    test  11  31  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   88.5  85.6  76.6  84.5  71.9  79.5     82.7   0.173  \n",
       "3   88.7  85.5  76.6    85  71.8  79.3     82.7  0.1728  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 1    # tanh\n",
    "hiddenUnits_LL = 4\n",
    "hiddenLayers_LL = 2\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same neural network characteristics used so far, we try to use a ReLu activation function instead. (The number of hidden units and layers are still randomized here). The information is summarized below for the train and test data. We see that the observations that can be made here are identical to the observations made about the first set of models that use the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  27   6  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "1      0    test  27   6  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "2      1   train  33  12  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "3      1    test  33  12  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "4      2   train  10  13  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "5      2    test  10  13  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "6      3   train  14  13  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "7      3    test  14  13  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "8      4   train  27  11  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "9      4    test  27  11  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "10     5   train  15  29  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "11     5    test  15  29  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 2    # relu\n",
    "hiddenUnits_LL = 4\n",
    "hiddenLayers_LL = 2\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same neural network characteristics used so far, we try to use a leaky ReLu activation function instead. (The number of hidden units and layers are still randomized here). The information is summarized below for the train and test data. We see that the observations that can be made here are identical to the observations made about the first set of models that use the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>10</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  28  20  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "1      0    test  28  20  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "2      1   train  36   8  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "3      1    test  36   8  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "4      2   train  26   5  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "5      2    test  26   5  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "6      3   train  25  21  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "7      3    test  25  21  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "8      4   train  10  39  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "9      4    test  10  39  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "10     5   train  35  31  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "11     5    test  35  31  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 3  # leaky relu    \n",
    "hiddenUnits_LL = 4\n",
    "hiddenLayers_LL = 2\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Number of Hidden Units and Layers Tunning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial models, we saw that we need to explore a model structure with a large number of hidden units, and a relalively small number of hidden layers. This observation was made when looking at the models that use the tanh function. The other models did not show any significant indications as to what model structure is better. Below we re-fit 6 models with each of the sigmoid, tanh, relu, and leaky relu activation functions in an attempt to increase model accuracy, improve prediction capability, and reduce MSE from the train and test dataset. This approach is taken to reduce any presence of underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we fit 6 models using the sigmoid activation function, with learnRate = 3, keepProb = 0.6, and 10 iterations. The number of hidden layers were randomly chosen, such that they are reasonaly less than the number of hidden units in a layer. The total accuracy for the models is still about 82% and almost the same MSE as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.254604\n",
      "Cost after iteration 0: 3.253265\n",
      "Cost after iteration 0: 3.252980\n",
      "Cost after iteration 0: 3.254291\n",
      "Cost after iteration 0: 3.253270\n",
      "Cost after iteration 0: 3.252078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>89.7</td>\n",
       "      <td>88.8</td>\n",
       "      <td>90</td>\n",
       "      <td>86.7</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.1</td>\n",
       "      <td>0.1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>89.7</td>\n",
       "      <td>88.7</td>\n",
       "      <td>89.9</td>\n",
       "      <td>86.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>13.5</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82</td>\n",
       "      <td>0.1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>88.6</td>\n",
       "      <td>19.5</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>82.5</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>65</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>88.7</td>\n",
       "      <td>19.9</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.9</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.7</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>75</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>76</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>86.4</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>76</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>86.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>13.9</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>87.4</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0</td>\n",
       "      <td>90.1</td>\n",
       "      <td>14</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>87.5</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  78  10  3     10  0.6        0  90.1  11.2  90.1  89.8   \n",
       "1      0    test  78  10  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "2      1   train  51  15  3     10  0.6        0  89.7  88.8    90  86.7   \n",
       "3      1    test  51  15  3     10  0.6        0  89.7  88.7  89.9  86.8   \n",
       "4      2   train  65  28  3     10  0.6        0  88.6  19.5  90.1  89.8   \n",
       "5      2    test  65  28  3     10  0.6        0  88.7  19.9  89.9  89.8   \n",
       "6      3   train  75  43  3     10  0.6        0  90.1  11.3  90.1  89.7   \n",
       "7      3    test  75  43  3     10  0.6        0  90.1  11.3  89.9  89.8   \n",
       "8      4   train  76  31  3     10  0.6        0  90.1  14.8  90.1  89.8   \n",
       "9      4    test  76  31  3     10  0.6        0  90.1  14.8  89.9  89.8   \n",
       "10     5   train  71   9  3     10  0.6        0  90.1  13.9  90.1  89.8   \n",
       "11     5    test  71   9  3     10  0.6        0  90.1    14  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1775  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  14.2  90.3  90.1     82.1  0.1787  \n",
       "3   90.4  90.8  90.1  13.5  90.2  89.9       82  0.1799  \n",
       "4   90.2  91.1  90.2  82.5  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1  82.9  90.2  89.9     82.3  0.1773  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   86.4  91.1  90.2  89.3  90.3  90.1     82.2  0.1777  \n",
       "9   86.6  90.8  90.1    90  90.2  89.9     82.2  0.1777  \n",
       "10  90.2  91.1  90.2  89.3  87.4  90.1     82.2  0.1777  \n",
       "11  90.4  90.8    90    90  87.5  89.9     82.2  0.1775  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 0    # sigmoid\n",
    "hiddenUnits_LL = 40\n",
    "hiddenLayers_LL = 4\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10  \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we fit 6 models using the tanh activation function, with learnRate = 3, keepProb = 0.6, and 10 iterations. The number of hidden layers were randomly chosen, such that they are reasonaly less than the number of hidden units in a layer. The total accuracy for the models is still about 82% and almost the same MSE as before. It does not appear that the difference between the number of hidden units and hidden layers made any improvements to the model. \n",
    "\n",
    "This is also the case with the models that use the Relu and leaky Relu activation functions in the next two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250829\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>78</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>50</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>62</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>62</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>68</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>68</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  70   6  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "1      0    test  70   6  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "2      1   train  63  39  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "3      1    test  63  39  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "4      2   train  78   8  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "5      2    test  78   8  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "6      3   train  50  36  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "7      3    test  50  36  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "8      4   train  62  25  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "9      4    test  62  25  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "10     5   train  68  32  3     10  0.6        1  90.1  11.2  90.1  89.8   \n",
       "11     5    test  68  32  3     10  0.6        1  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 1    # tanh\n",
    "hiddenUnits_LL = 40\n",
    "hiddenLayers_LL = 4\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>63</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>63</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  43  26  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "1      0    test  43  26  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "2      1   train  45  45  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "3      1    test  45  45  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "4      2   train  63  31  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "5      2    test  63  31  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "6      3   train  69  32  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "7      3    test  69  32  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "8      4   train  57  25  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "9      4    test  57  25  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "10     5   train  57  23  3     10  0.6        2  90.1  11.2  90.1  89.8   \n",
       "11     5    test  57  23  3     10  0.6        2  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 2    # relu\n",
    "hiddenUnits_LL = 40\n",
    "hiddenLayers_LL = 10\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n",
      "Cost after iteration 0: 3.250830\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>56</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>56</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>79</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>79</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>77</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>77</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>train</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>91.1</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>82.2</td>\n",
       "      <td>0.1776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>test</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3</td>\n",
       "      <td>90.1</td>\n",
       "      <td>11.3</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>90.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>90</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.9</td>\n",
       "      <td>82.3</td>\n",
       "      <td>0.1774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model dataset  HU  HL LR nbIter   KP Function     0     1     2     3  \\\n",
       "0      0   train  61  25  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "1      0    test  61  25  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "2      1   train  56  15  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "3      1    test  56  15  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "4      2   train  79  23  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "5      2    test  79  23  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "6      3   train  77  34  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "7      3    test  77  34  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "8      4   train  48  15  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "9      4    test  48  15  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "10     5   train  65  31  3     10  0.6        3  90.1  11.2  90.1  89.8   \n",
       "11     5    test  65  31  3     10  0.6        3  90.1  11.3  89.9  89.8   \n",
       "\n",
       "       4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "1   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "2   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "3   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "4   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "5   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "6   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "7   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "8   90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "9   90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  \n",
       "10  90.2  91.1  90.2  89.3  90.3  90.1     82.2  0.1776  \n",
       "11  90.4  90.8  90.1    90  90.2  89.9     82.3  0.1774  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 3  # leaky relu    \n",
    "hiddenUnits_LL = 40\n",
    "hiddenLayers_LL = 10\n",
    "learnRate = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 10 \n",
    "modelStructure(function, hiddenUnits_LL, hiddenLayers_LL, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MSE scores for the train and test dataset are close to each other, in the previous models. For now, we can assume that this closeness indicates little to no overfitting. So, we will try to improve the accuracy of our best model structure so far by increasing the number of iterations. \n",
    "\n",
    "The best model so far has:\n",
    "- an overall accuracy of 82.7%\n",
    "- train MSE= 0.173 and test MSE= 0.1728\n",
    "- uses the tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "- learning rate equal to 3\n",
    "- inverted dropout keep_prob parameter = 0.6\n",
    "- 10 iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Number of Iterations Tuning:\n",
    "\n",
    "Moving forward, we will explore and try to improve on the best model so far, with the following structure:\n",
    "- uses the tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "\n",
    "As mentioned before, we are restiricted by time and computer power. The activation functions sigmoid, relu, and leaky relu showed similar results, but nothing exceptional. Although we will not continue exploring them in this assignment, it does not mean better models cannot be found with these activation funcitons.\n",
    "\n",
    "We will set the learning rate = 0.9 and the inverted dropout keep_prob parameter = 0.6, untill we get to tuning them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learnIter : creates a table with summarized information for a neural network it fits with a fixed structure, but different nb of iterations\n",
    "# function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "# hiddenUnits : number of hidden units in a layer\n",
    "# hiddenLayers : number of hidden layers\n",
    "# learnRate : learning rate \n",
    "# keepProb : inverted dropout regularization parameter\n",
    "# nbIterations : integer, numbers of iterations to train the models with\n",
    "# Modelsummary : table that contains the basic information about the neural network structure, and \n",
    "#                 the accuracy scores and MSE scores for the train and test data\n",
    "\n",
    "def learnIter(function, hiddenUnits, hiddenLayers, learnRate, keepProb, nbIterations):\n",
    "    \n",
    "    # contains the basic information about the neural network structure\n",
    "    # Model :  model name signified by a number (or model index)\n",
    "    # dataset : wether the information in the row is for the training or test data\n",
    "    # HU : number of hidden units in a layer\n",
    "    # HL : number of hidden layers\n",
    "    # LR : learning rate\n",
    "    # nbIter : number of iterations\n",
    "    # KP : keepProb : inverted dropout regularization parameter\n",
    "    # Function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "    Modelsummary = pd.DataFrame(columns  = ['Model','dataset', 'HU','HL', 'LR','nbIter','KP','Function'])\n",
    "    \n",
    "    # will contain the accuracy scores and MSE scores for the train and test data\n",
    "    AnalysisSummary = pd.DataFrame()\n",
    "\n",
    "    k = 0    # counter for the model being fitted\n",
    "    Modelsummary.loc[len(Modelsummary.index)] = [str(k),'train', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "    Modelsummary.loc[len(Modelsummary.index)] = [str(k),'test', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "\n",
    "    # training model\n",
    "    params = nn_model(X_train, y_train, n_h = hiddenUnits, num_iterations = nbIterations, print_cost=True, actFun = function, n_hl=hiddenLayers, keep_prob=keepProb, learn_rate = learnRate)\n",
    "\n",
    "    # Computing the total accuracy (%), accuracy for each digit, and MSE\n",
    "    tr_pred = predict(params, X_train, function, keep_prob = keepProb)    # test dataset\n",
    "    tr = getAccuracy_MSE(tr_pred, y_train, str(k))\n",
    "    tr.at[0, 'dataset'] = 'train'\n",
    "\n",
    "    te_pred = predict(params, X_test, function, keep_prob = keepProb)     # train dataset\n",
    "    te = getAccuracy_MSE(te_pred, y_test, str(k))\n",
    "    te.at[0, 'dataset'] = 'test'\n",
    "\n",
    "    AnalysisSummary = pd.concat([AnalysisSummary, (pd.concat([tr, te], axis=0))], axis=0) \n",
    "    \n",
    "    Modelsummary = pd.merge(Modelsummary, AnalysisSummary, on = ['Model','dataset'])\n",
    "        \n",
    "    return Modelsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see the cost of the model for every 100 iterations. We took a maximum of 200 iterations. The cost decreased drastically from 10 to 200 iterations. Additionally, the model's accuracy is about 98% for the test data, with an overall high accuracy for each of the digits. The MSE is smaller that what we had before, so the additional iterations did improve the model. Lastly, the MSE for the train and test datasets are close to each other and small, showing that there is little signs of overfitting and underfitting.\n",
    "\n",
    "Before deducing the best model, let us try running some hyperparameter tunning for the learning rate and the inverted dropout keep_prob parameter, with 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250680\n",
      "Cost after iteration 100: 0.454086\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>200</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>99.3</td>\n",
       "      <td>98.8</td>\n",
       "      <td>98.1</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.2</td>\n",
       "      <td>99.3</td>\n",
       "      <td>98.7</td>\n",
       "      <td>98.3</td>\n",
       "      <td>98.2</td>\n",
       "      <td>98.7</td>\n",
       "      <td>0.0131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>200</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>99.2</td>\n",
       "      <td>98.5</td>\n",
       "      <td>97.8</td>\n",
       "      <td>98.5</td>\n",
       "      <td>97.7</td>\n",
       "      <td>99</td>\n",
       "      <td>98.4</td>\n",
       "      <td>98</td>\n",
       "      <td>97.9</td>\n",
       "      <td>98.4</td>\n",
       "      <td>0.0156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model dataset  HU HL   LR nbIter   KP Function     0     1     2     3  \\\n",
       "0     0   train  41  3  0.9    200  0.6        1  99.3  99.3  98.8  98.1   \n",
       "1     0    test  41  3  0.9    200  0.6        1  99.3  99.2  98.5  97.8   \n",
       "\n",
       "      4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0  98.9  98.2  99.3  98.7  98.3  98.2     98.7  0.0131  \n",
       "1  98.5  97.7    99  98.4    98  97.9     98.4  0.0156  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 1    # tanh\n",
    "hiddenUnits = 41\n",
    "hiddenLayers = 3\n",
    "learnRate = 0.9\n",
    "keepProb = 0.6\n",
    "nbIter = 200\n",
    "learnIter(function, hiddenUnits, hiddenLayers, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Learning Rate and Regularization Tunning:\n",
    "\n",
    "Building on the best model so far, this section explores different random combinations of learning rates and regularization paramter values for the model with:\n",
    "- tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "- 200 iterations\n",
    "\n",
    "This model structure is the best so far, since it has the highest train and test accuracies of 98.7% and 98.4% respectively, with small MSE values.\n",
    "When we trained the model with 200 iterations, the train and test MSE were very close to each other with a difference of 0.0025. However, the difference between the train and test MSE was 0.0002, when the model was trained with 10 iterations. This shows that the variance did slightly increase. To decrease the variance, we will look at inverted dropout keep_prob parameters less than 0.6, which is what we used before. This way, the inverted dropout regularization method with keep less than 60% of the units in each internal layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixLearningRate_reg : creates a table with summarized information for a neural network it fits with different learning rates and regularization parameter values\n",
    "# function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "# hiddenUnits : number of hidden units in a layer\n",
    "# hiddenLayers : number of hidden layers\n",
    "# keepProb_UpperLimit : upper limit for the inverted dropout regularization parameter, at most should be equal to 1\n",
    "# nbIterations : integer, numbers of iterations to train the models with\n",
    "# learning rate option is a randomized number in the interval [0,0.9)\n",
    "# keepProb regularization parameter is a randomized number in the interval [0.3, keepProb_UpperLimit)\n",
    "# Modelsummary : table that contains the basic information about the neural network structure, and \n",
    "#                 the accuracy scores and MSE scores for the train and test data\n",
    "\n",
    "def fixLearningRate_reg(function, hiddenUnits, hiddenLayers, keepProb_UpperLimit , nbIterations):\n",
    "    \n",
    "    # contains the basic information about the neural network structure\n",
    "    # Model :  model name signified by a number (or model index)\n",
    "    # dataset : wether the information in the row is for the training or test data\n",
    "    # HU : number of hidden units in a layer\n",
    "    # HL : number of hidden layers\n",
    "    # LR : learning rate\n",
    "    # nbIter : number of iterations\n",
    "    # KP : keepProb : inverted dropout regularization parameter\n",
    "    # Function: the activation function to use in the model: 0 (sigmid) ,1(tanh),2(relu),3(leaky relu)\n",
    "    Modelsummary = pd.DataFrame(columns  = ['Model','dataset', 'HU','HL', 'LR','nbIter','KP','Function'])\n",
    "    \n",
    "    # will contain the accuracy scores and MSE scores for the train and test data\n",
    "    AnalysisSummary = pd.DataFrame()\n",
    "\n",
    "    for k in range(0,5):       # training 5 models with randomized learning rate and regularization values\n",
    "        np.random.seed(np.random.randint(100000, size=5)[k])\n",
    "        learnRate = round(np.random.uniform(0, 0.9),3)\n",
    "        keepProb = round(np.random.uniform(0.3, keepProb_UpperLimit),3)\n",
    "        Modelsummary.loc[len(Modelsummary.index)] = [str(k),'train', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "        Modelsummary.loc[len(Modelsummary.index)] = [str(k),'test', hiddenUnits, hiddenLayers, learnRate, nbIterations, keepProb, function]\n",
    "\n",
    "        # training model\n",
    "        params = nn_model(X_train, y_train, n_h = hiddenUnits, num_iterations = nbIterations, print_cost=True, actFun = function, n_hl=hiddenLayers, keep_prob=keepProb, learn_rate = learnRate)\n",
    "\n",
    "        # Computing the total accuracy (%), accuracy for each digit, and MSE\n",
    "        tr_pred = predict(params, X_train, function, keep_prob = keepProb)     # train dataset\n",
    "        tr = getAccuracy_MSE(tr_pred, y_train, str(k))\n",
    "        tr.at[0, 'dataset'] = 'train'\n",
    "\n",
    "        te_pred = predict(params, X_test, function, keep_prob = keepProb)      # test dataset\n",
    "        te = getAccuracy_MSE(te_pred, y_test, str(k))\n",
    "        te.at[0, 'dataset'] = 'test'\n",
    "\n",
    "        AnalysisSummary = pd.concat([AnalysisSummary, (pd.concat([tr, te], axis=0))], axis=0)                        \n",
    "    \n",
    "    Modelsummary = pd.merge(Modelsummary, AnalysisSummary, on = ['Model','dataset'])\n",
    "        \n",
    "    return Modelsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below gives the summary for all the different models. The total accuracy for the first 4 models is not better than the 98.7% train datast and 98.4% test dataset accuracy found with learnRate = 0.9 and keepProb = 0.6. Similarly, their MSE values are not smaller nor closer to what we had before. \n",
    "\n",
    "The last model (model 4) has a 98.6% train datast and 98.4% test dataset accuracy, with a 0.0024 MSE difference. This model has slightly less accuracy with the train dataset and slightly larger MSE values, compared to the previous model with learnRate = 0.9 and keepProb = 0.6. It appears that the model preforms better with a learning rate around 0.8 or 0.9, while dropping around 60% of the units in each hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250658\n",
      "Cost after iteration 100: 1.030961\n",
      "Cost after iteration 0: 3.250612\n",
      "Cost after iteration 100: 0.818649\n",
      "Cost after iteration 0: 3.250650\n",
      "Cost after iteration 100: 2.477276\n",
      "Cost after iteration 0: 3.250622\n",
      "Cost after iteration 100: 0.733049\n",
      "Cost after iteration 0: 3.250675\n",
      "Cost after iteration 100: 0.489039\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.326</td>\n",
       "      <td>200</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.3</td>\n",
       "      <td>97.7</td>\n",
       "      <td>98.1</td>\n",
       "      <td>97.6</td>\n",
       "      <td>98.8</td>\n",
       "      <td>98.4</td>\n",
       "      <td>97.4</td>\n",
       "      <td>97.2</td>\n",
       "      <td>98.1</td>\n",
       "      <td>0.0186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.326</td>\n",
       "      <td>200</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.2</td>\n",
       "      <td>97.5</td>\n",
       "      <td>97.9</td>\n",
       "      <td>97.5</td>\n",
       "      <td>98.7</td>\n",
       "      <td>98.2</td>\n",
       "      <td>97.4</td>\n",
       "      <td>97.2</td>\n",
       "      <td>98</td>\n",
       "      <td>0.0196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.398</td>\n",
       "      <td>200</td>\n",
       "      <td>0.374</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>88.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.2</td>\n",
       "      <td>52.2</td>\n",
       "      <td>90.2</td>\n",
       "      <td>89.3</td>\n",
       "      <td>46.4</td>\n",
       "      <td>90.1</td>\n",
       "      <td>81.7</td>\n",
       "      <td>0.1828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.398</td>\n",
       "      <td>200</td>\n",
       "      <td>0.374</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>88.7</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.8</td>\n",
       "      <td>90.4</td>\n",
       "      <td>52.1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.9</td>\n",
       "      <td>47</td>\n",
       "      <td>89.9</td>\n",
       "      <td>81.8</td>\n",
       "      <td>0.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.154</td>\n",
       "      <td>200</td>\n",
       "      <td>0.472</td>\n",
       "      <td>1</td>\n",
       "      <td>98.1</td>\n",
       "      <td>97.8</td>\n",
       "      <td>97.2</td>\n",
       "      <td>95.7</td>\n",
       "      <td>96.2</td>\n",
       "      <td>95.9</td>\n",
       "      <td>98.1</td>\n",
       "      <td>97</td>\n",
       "      <td>95.2</td>\n",
       "      <td>94.8</td>\n",
       "      <td>96.6</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.154</td>\n",
       "      <td>200</td>\n",
       "      <td>0.472</td>\n",
       "      <td>1</td>\n",
       "      <td>98.2</td>\n",
       "      <td>97.9</td>\n",
       "      <td>97</td>\n",
       "      <td>95.5</td>\n",
       "      <td>96.1</td>\n",
       "      <td>96</td>\n",
       "      <td>98.1</td>\n",
       "      <td>97.1</td>\n",
       "      <td>95.2</td>\n",
       "      <td>95.1</td>\n",
       "      <td>96.6</td>\n",
       "      <td>0.0338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.458</td>\n",
       "      <td>200</td>\n",
       "      <td>0.396</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>88.8</td>\n",
       "      <td>90.1</td>\n",
       "      <td>89.3</td>\n",
       "      <td>90.2</td>\n",
       "      <td>53.1</td>\n",
       "      <td>89.5</td>\n",
       "      <td>86.9</td>\n",
       "      <td>53.4</td>\n",
       "      <td>89.1</td>\n",
       "      <td>82</td>\n",
       "      <td>0.1797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.458</td>\n",
       "      <td>200</td>\n",
       "      <td>0.396</td>\n",
       "      <td>1</td>\n",
       "      <td>90.1</td>\n",
       "      <td>88.7</td>\n",
       "      <td>89.9</td>\n",
       "      <td>89.2</td>\n",
       "      <td>90.4</td>\n",
       "      <td>52.7</td>\n",
       "      <td>89.3</td>\n",
       "      <td>87.4</td>\n",
       "      <td>53</td>\n",
       "      <td>89.1</td>\n",
       "      <td>82</td>\n",
       "      <td>0.1801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>99.3</td>\n",
       "      <td>98.7</td>\n",
       "      <td>98</td>\n",
       "      <td>98.8</td>\n",
       "      <td>98.1</td>\n",
       "      <td>99.2</td>\n",
       "      <td>98.7</td>\n",
       "      <td>98.2</td>\n",
       "      <td>98.1</td>\n",
       "      <td>98.6</td>\n",
       "      <td>0.0136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.578</td>\n",
       "      <td>1</td>\n",
       "      <td>99.2</td>\n",
       "      <td>99.2</td>\n",
       "      <td>98.4</td>\n",
       "      <td>97.8</td>\n",
       "      <td>98.5</td>\n",
       "      <td>97.7</td>\n",
       "      <td>99</td>\n",
       "      <td>98.3</td>\n",
       "      <td>98</td>\n",
       "      <td>97.8</td>\n",
       "      <td>98.4</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model dataset  HU HL     LR nbIter     KP Function     0     1     2     3  \\\n",
       "0     0   train  41  3  0.326    200  0.497        1  98.9  98.9  98.3  97.7   \n",
       "1     0    test  41  3  0.326    200  0.497        1  98.9  98.9  98.2  97.5   \n",
       "2     1   train  41  3  0.398    200  0.374        1  90.1  88.8  90.1  89.8   \n",
       "3     1    test  41  3  0.398    200  0.374        1  90.1  88.7  89.9  89.8   \n",
       "4     2   train  41  3  0.154    200  0.472        1  98.1  97.8  97.2  95.7   \n",
       "5     2    test  41  3  0.154    200  0.472        1  98.2  97.9    97  95.5   \n",
       "6     3   train  41  3  0.458    200  0.396        1  90.1  88.8  90.1  89.3   \n",
       "7     3    test  41  3  0.458    200  0.396        1  90.1  88.7  89.9  89.2   \n",
       "8     4   train  41  3  0.799    200  0.578        1  99.3  99.3  98.7    98   \n",
       "9     4    test  41  3  0.799    200  0.578        1  99.2  99.2  98.4  97.8   \n",
       "\n",
       "      4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0  98.1  97.6  98.8  98.4  97.4  97.2     98.1  0.0186  \n",
       "1  97.9  97.5  98.7  98.2  97.4  97.2       98  0.0196  \n",
       "2  90.2  52.2  90.2  89.3  46.4  90.1     81.7  0.1828  \n",
       "3  90.4  52.1  90.1  89.9    47  89.9     81.8   0.182  \n",
       "4  96.2  95.9  98.1    97  95.2  94.8     96.6   0.034  \n",
       "5  96.1    96  98.1  97.1  95.2  95.1     96.6  0.0338  \n",
       "6  90.2  53.1  89.5  86.9  53.4  89.1       82  0.1797  \n",
       "7  90.4  52.7  89.3  87.4    53  89.1       82  0.1801  \n",
       "8  98.8  98.1  99.2  98.7  98.2  98.1     98.6  0.0136  \n",
       "9  98.5  97.7    99  98.3    98  97.8     98.4   0.016  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 1    # tanh\n",
    "hiddenUnits = 41\n",
    "hiddenLayers = 3\n",
    "keepProb = 0.6\n",
    "nbIter = 200  \n",
    "fixLearningRate_reg(function, hiddenUnits, hiddenLayers, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a last attempt to improve the model, a model with the characteristics below is fitted:\n",
    "- tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "- 201 iterations\n",
    "- learnRate = 0.8\n",
    "- keepProb = 0.6\n",
    "\n",
    "The learning rate was taken to be 0.8 since we established that a learnRate of about 0.8, coupled with the above structure characteristics, yields a model with high accuracies. Taking a learning rate = 0.799 is too specific, so learnRate = 0.8 may help the neural network be generalized.\n",
    "The resulting model has the best accuracies so far with 98.9% train datast and 98.6% test dataset accuracy. Additionally, it has a 0.0115 - 0.0138 = 0.0023 MSE difference.\n",
    "\n",
    "The best model we had before with the characteristics below:\n",
    "- tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "- 200 iterations\n",
    "- learnRate = 0.9\n",
    "- keepProb = 0.6\n",
    "\n",
    "has a 98.7% train datast and 98.4% test dataset accuracy, and a 0.0131 - 0.0156 = 0.0025 MSE difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.250680\n",
      "Cost after iteration 100: 0.485554\n",
      "Cost after iteration 200: 0.348898\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>dataset</th>\n",
       "      <th>HU</th>\n",
       "      <th>HL</th>\n",
       "      <th>LR</th>\n",
       "      <th>nbIter</th>\n",
       "      <th>KP</th>\n",
       "      <th>Function</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>TotalAcc</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>201</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>99.4</td>\n",
       "      <td>99.4</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.4</td>\n",
       "      <td>99</td>\n",
       "      <td>98.5</td>\n",
       "      <td>99.4</td>\n",
       "      <td>98.9</td>\n",
       "      <td>98.5</td>\n",
       "      <td>98.3</td>\n",
       "      <td>98.9</td>\n",
       "      <td>0.0115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>0.8</td>\n",
       "      <td>201</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>99.3</td>\n",
       "      <td>99.3</td>\n",
       "      <td>98.6</td>\n",
       "      <td>98.1</td>\n",
       "      <td>98.6</td>\n",
       "      <td>98.2</td>\n",
       "      <td>99.1</td>\n",
       "      <td>98.6</td>\n",
       "      <td>98.2</td>\n",
       "      <td>98.2</td>\n",
       "      <td>98.6</td>\n",
       "      <td>0.0138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model dataset  HU HL   LR nbIter   KP Function     0     1     2     3  \\\n",
       "0     0   train  41  3  0.8    201  0.6        1  99.4  99.4  98.9  98.4   \n",
       "1     0    test  41  3  0.8    201  0.6        1  99.3  99.3  98.6  98.1   \n",
       "\n",
       "      4     5     6     7     8     9 TotalAcc     MSE  \n",
       "0    99  98.5  99.4  98.9  98.5  98.3     98.9  0.0115  \n",
       "1  98.6  98.2  99.1  98.6  98.2  98.2     98.6  0.0138  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function = 1    # tanh\n",
    "hiddenUnits = 41\n",
    "hiddenLayers = 3\n",
    "learnRate = 0.8\n",
    "keepProb = 0.6\n",
    "nbIter = 201\n",
    "learnIter(function, hiddenUnits, hiddenLayers, learnRate, keepProb, nbIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The neural network with the characteristics below is the best model found in this assignment:\n",
    "- tanh activation function\n",
    "- 41 hidden units\n",
    "- 3 hidden layers\n",
    "- 201 iterations\n",
    "- learnRate = 0.8\n",
    "- keepProb = 0.6\n",
    "\n",
    "This model has the best accuracies with 98.9% train datast and 98.6% test dataset accuracy, coupled with the smallest train and test MSE values of 0.0115, 0.0138 respectively. The MSE values are small and close, which shows little to no signs of underfitting. The MSE difference of 0.0023 is small, which shows little to no signs of overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Questions and Difficulties:\n",
    "\n",
    "This assignment does not take the best approach to tune all 6 hyperparameters: activation function, nb of hidden units, nb if hidden layers, learning rate, nb of iterations, and the inverted dropout regularization paramter. This is due to time constraints and limited computing power. By applying randomized grid searches, other well-performing and more efficient neural networks may be found. For the same limitations, cross-validation was not performed. However, our confidence in the generalizability of the neural network would be higher if we apply cross-validation.\n",
    "\n",
    "The inner layers of the neural network, in this assignment, where all taken to have the same number of units and dimensions, for simplicity. Other neural network structures can be explored that have varying number of units in each layer.\n",
    "\n",
    "Although a neural netowrk was found by increasing the number of iterations, a large number of iterations was not tried. This led to reducing the cost, without seeing a turning point for when it starts to increase. This means that the neural netowrk that was found may still be improved if more iterations are run. However, this is was not further explored in the assignment, since the accuracy was already very high, and the train and test MSE statrted to deviate from each other as the number of iterations increased, which signify overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography:\n",
    "\n",
    "- This report builds and adds on the code from the 'Backpropagation, NN_backpropagation' file provided in the CS4120 moodle course page.\n",
    "\n",
    "Bolufe-Rohler, Anotnio. CS 4120, Machine Learning and Data Minig Course. Concepts taken from lectures: \n",
    "- 'Lecture 12 - Intro Neural Networks'\n",
    "- 'Lecture 13 - Neural Networks Implementation'\n",
    "- 'Lecture 14 - Deep Neural Networks'\n",
    "- 'Lecture 16 - Important Concept Deep Learning'\n",
    "- 'Lecture 17 - Deep Learning Optimization'"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN-backpropagation.ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
